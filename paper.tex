%%this file is to be a minimal file for using the IEEE journal
%%Austin F. Oltmanns
%%Anna Titova

\documentclass[journal]{IEEEtran}

\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  \graphicspath{(./)}
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\fi

\usepackage{amsmath}
\usepackage{textcomp,xspace}
\newcommand\la{\textlangle\xspace}
\newcommand\ra{\textrangle\xspace}
\usepackage{amssymb}
\usepackage{hyperref}

\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\title{Mutual Coherence Minimization \\via Projection Matrices\\ for CSM EENG 515}


\author{ 
Anna~Titova \\
Austin~F.~Oltmanns% <-this % stops a space
}

\maketitle


\begin{abstract}
Mutual coherence (MC) determines how related the vectors of a sensing matrix are. 
If it is possible to reduce the MC of a sensing matrix sufficiently, then it is
possible to use that matrix in compressive sensing applications such as the
construction of an adequate dictionary for the sparse representation of signals.
Mutual coherence minimization of arbitrary sensor arrays is achieved 
by multiplying the predetermined sensing matrix by a set of gains
determined by an algorithm. Additionally, direct mutual coherence minimization is explored
through an example and an algorithm is given.
\end{abstract}
\begin{IEEEkeywords} compressive sensing,  $\ell_1$ minimization, mutual coherence, projection operator,
sensing matrix, sparse dictionary, sparse signals
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{T}{he} reduction of Mutual Coherence (MC) of sensing systems has long plagued
those who need to measure quantities. It is important that if multiple measurements
are taken, that there is not too much redundant information.
Through Compressive Sensing (CS) techniques, it is possible to achieve
a smaller amount of required measurements. Typically, these techniques involve a sparse
sampling of an over-complete dictionary which is able to describe the signal being sampled. This 
motivates the question: What is the "best" sparse sampling?

This paper examines one method in particular for determining optimal sparse signal 
dictionaries and sampling matrices. First mutual coherence minimization is explored as well
as the relevant mathematics required for MC minimization. Then a lower dimensional example
of MC minimization is covered followed by a higher dimensional example involving MC reduction
via a projection matrix. Finally, this method is compared to other current methods.

\hfill December 11, 2018
\section{Background}
There are many ways to measure a signal, especially when the signal is a two dimensional array. In this
paper, several norms are used, each of which treats the input as a vector. First we define the matrix $\ell_\infty$ norm:
\begin{equation}
||A||_\infty = \max_{i,j} a_{ij}, \quad A \in \mathbb{R}^{m\times n}.
\label{eq:linfty}
\end{equation}
Note that this is simply the largest entry in the matrix. This should not be confused with any other definition of the matrix $\ell_\infty$ norm, such as the one which takes
the largest absolute row sum. Next we define the Frobenius norm:
\begin{equation}
||A||_F = \sqrt{\sum_{i,j} |a_{ij}|^2}, \quad A \in \mathbb{R}^{m\times n}
\label{eq:frob}
\end{equation}
This can be thought of as vectorizing $A$, then computing its vector $\ell_2$ norm.  To define the vector 
$\ell_2$ norm:
\begin{equation}
||x||_2 = \sqrt{\sum_{i} |x_{i}|^2}, \quad X \in \mathbb{R}^{n}.
\label{eq:vecl2}
\end{equation}
Equation~\ref{eq:vecl2} is a commonly used norm and is a member of the $\ell_p$ norms,  where $1\leq p < \infty$. 

Another norm from this family of norms is used, the $\ell_1$ norm:
\begin{equation}
||A||_1 = \sum_{i,j} |a_{ij}|, \quad A \in \mathbb{R}^{m\times n}.
\label{eq:l1}
\end{equation}
Again, this definition should not be confused with the definition of the $\ell_1$ norm which yields the largest absolute column sum. This definition first vectorizes the input matrix then computes $\ell_1$
norm. To define the vector $\ell_1$ norm:
\begin{equation}
||x||_1 = \sum_{i} |x_{i}|, \quad X \in \mathbb{R}^{n}.
\label{eq:vecl1}
\end{equation}

The last norm which is used throughout this paper is the $\ell_0$ norm, which is non-convex and does not belong to the  $\ell_p$ norms family, where $1\leq p < \infty$.  This 
norm is defined as counting the number of zero entries of a vector. This norm is used when determining
how sparse a signal is. 
A typical example of measuring a sparse signal arises when attempting to represent some $x$ 
as a sparse signal $\alpha$ using an overcomplete dictionary, $D$:
\begin{align}
\alpha = Dx, \quad \mathrm{s.t.} \quad \min ||\alpha||_0
\label{eq:sparseproblem}
\end{align}
Because this problem uses the $\ell_0$ norm, the problem is not immediately tractable. Some transformations
must be made to the problem to provide a solution other than brute force checking of every possible
$\alpha$. 

One method for transforming this problem, involves transitioning from the $\ell_0$ norm to the $\ell_1$ norm.
The solutions to these two problems coincide when the mutual coherence of the dictionary used is low 
enough \cite{Lin}. The MC of a matrix may be calculated via:
\begin{align}
\mu(M)=\max\limits_{\substack{1\leq i,j \leq n \\ i\neq j}}\frac{|\langle (M)_i, (M)_j \rangle |}{\|(M)_i\| \|(M)_j\|},
\label{Eq:muchar1}
\end{align}
where $ (M)_i$ denotes the $i$-th column of $M$ and $\langle\cdot , \cdot \rangle $ denotes the standard inner product in $\mathbb{R}^n$. 
Alternatively, Equation~\ref{Eq:muchar1} may be written as:
\begin{align}
    \mu(M) = ||M^T M - I||_\infty.
\end{align}

This can be thought of as the smallest angle between any two vectors in the dictionary (noting that
the absolute value around the inner product restricts the angles to one side only). 

This quantity
has a lower bound known as the Welch bound determined by the dimension of the dictionary:
\begin{equation}
\mu_{WB}(M) \geq \sqrt{\frac{n - m}{m(n-1)}}.
\end{equation}
It should be noted that this is not the greatest lower bound of MC, and that it is not always
achieved. The true lower limit of MC for a particular dictionary depends only on its dimension. This will be demonstrated in the higher dimensional examples.

There have been different techniques used to minimize MC recently, some of which include: Elad's algorithm \cite{Elard}, the algorithm of Duarte-Carajalino and Sapiro \cite{Sapiro}, the algorithm of Xu et al \cite{Xu}, the algorithm of Lin et al. \cite{Lin}, and the algorithm of Obermeier and Martinez-Lorenzo \cite{Obermeier}. The Fig.~\ref{Fig:comp_of_algtms} shows the comparison between performance of different algorithms as well as mutual coherence of a random matrix built by using the uniform distribution (the random matrices initially have small mutual coherence because their entries independent).  \\
\begin{figure}[!h]
\centering
\includegraphics[width=3in]{comparison_for_paper.pdf}
\caption{Plots of the averaged mutual coherence of a matrix of size $m \times n$ v.s. number of rows $m$ with fixed number of columns $n$ (From Lin et al. \cite{Lin})}
\label{Fig:comp_of_algtms}
\end{figure}
It can be seen in Fig.~\ref{Fig:comp_of_algtms} that the algorithm of Lin et al. has superior performance compared with other algorithms. Also it is worth noting that the Welch bound is not achieved by any algorithm for these dimensions of dictionaries.  Fig.~\ref{Fig:comp_of_algtms} does not include the algorithm of Obermeier and Martinez-Lorenzo, but as they mention in their papers, the Welch bound is also not achieved by them.

The algorithms proposed in the papers \cite{Elard}, \cite{Sapiro}, and \cite{Xu} use the properties 
of the Gram matrix to find matrices with low mutual coherence. Lin et al. propose to minimize mutual coherence directly by means of a proximal gradient method \cite{Lin}. 
The algorithm of Obermeier and Martinez-Lorenzo uses a differenct approach compared to  \cite{Elard}, \cite{Xu}, and \cite{Sapiro}. This method requires the columns of the sensing matrix be computed by a differentiable function. 
%the Lagrangian the Augmented Lagrangian method and
The paper by Lin et al., contains clear and detailed descriptions of the algorithms used and the material it covers coincides well with the material 
covered in CSM EENG515.

Two key methods used by Lin et al. are the projection onto the $\ell_1$ ball and a proximal gradient
method. The projection onto the $\ell_1$ ball used is that
proposed by Singer et al. \cite{l1_ball}. They formulate the problem as:
\begin{align}
   \min\limits_{\mathbf{w}\in\mathbb{R}^n} \|\mathbf{w}-\mathbf{v} \|^2_2 \quad \mathrm{s.t.} \quad \|\mathbf{w} \|_1 \leq r, 
\end{align}
where $r$ is the radius of the $\ell_1$ ball, $\mathbf{v}\in\mathbb{R}^n$ is the vector we would like to project onto the $\ell_1$ ball, and $\mathbf{w}$ is the result of the projection. The exact details
behind calculating $w$ are explained very well in \cite{l1_ball}, where they have included MATLAB code 
for computing the projection.

%Proximal Gradient Techniques: $\ldots$
The above mentioned $\ell_1$ projection technique is necessary for the proximal gradient method also
described by Lin et al. This method is
a generalized form of gradient descent which is applicable to functions which are not differentiable 
(such as the $\ell_0$ norm). In this method the objective function is able to be smoothed in some way. 
For example, suppose one wishes to maximize an inner product while maintaining an $\ell_1$ norm size 
constraint
\begin{align}
f(M) = \max_{||\mathbf{V}||_1 \leq 1} <M^T M - I, V>.
\end{align}
This $\ell_1$ norm constraint makes this a nondifferentiable expression, however, a smooth approximation
may be made through the subtraction of a so-called proximal function:\\ $d(V) = \frac{\rho}{2}||V||_F^2$:
\begin{align}
    f_\rho(M) = \max_{||\mathbf{V}||_1 \leq 1} <M^T M - I, V> - \frac{\rho}{2}||V||_F^2,
\end{align}
where the inner product between two matrices is defined as:
\begin{align}
    <A, B> = \mathrm{tr}(A^H B), \quad A,B \in \mathbb{R}^{m\times n},
\end{align}
and $\rho$ is a smoothing factor. For small enough $\rho$, $f_p$ may be made arbitrarily close to $f$. 
Further, $f$ is bounded from above and below as follows:
\begin{align}
    f_p \leq f \leq f_p + \frac{\rho}{2}(m \cdot n),
\end{align}
where $m,n$ are the dimensions of the matrix $M$.
The gradient (denoted by $\nabla$) of this smoothed function may be approximated by:
\begin{align}
    \nabla f_\rho(M) = M(V + V^T),
\end{align}
where $V$ is the projection of $(M^T M - I)/\rho$ onto the $\ell_1$ ball.
%In our project we used the algorithm that was proposed in the paper by Lin et al., 2018 (Reference). They call their algorithm as the Direct Mutual Coherence Minimization (DMCM) model. Such name  
%\pagebreak %optional
\section{Methods}
%\subsection{Projection onto $l_1$ ball}
\subsection{Mutual Coherence Minimization}
The method of Lin et al. for direct mutual coherence minimization involves estimating the gradient 
of the mutual coherence of the  dictionary.
\begin{align}
    \mu(M) &= ||M^T M - I||_\infty \\
     &= \max_{||\mathbf{V}|| \leq 1} <M^T M - I, V>
     \label{Eq:newmu}
\end{align}
The expression given for mutual coherence in Equation~\ref{Eq:newmu} may be smoothed and have its gradient
approximated as demonstrated in the background section. That is, by using a proximal function 
to smooth out the objective function and then projecting the result onto the $l_1$ ball. The smoothed
objective function is given as:
\begin{align}
    \mu_{\rho}(M) = \max_{||\mathbf{V}||_1 \leq 1} <M^T M - I, V> - \frac{\rho}{2}||V||_F^2.
\end{align}
This smoothed approximation has gradient:
\begin{align}
    \nabla \mu_\rho(M) = M(V + V^T),
\end{align}
where at each iteration, the factor $V$ is
the projection of $(M_k^T M_k - I)/ \rho $ onto the $l_1$ ball.
And to iterate for new columns of M, the equation of iteration is
\begin{align}
    (M_{k+1})_i = \frac{(M_k)_i - \alpha (\nabla \mu_\rho(M_k)_i}{||(M_k)_i 
    - \alpha (\nabla \mu_\rho M_k)_i||_2},
\label{eq:iterate}
\end{align}
where $\alpha$ is a scaling factor (a value of .99 works well for most cases). This method of first computing $V$ and then iterating $M$ using
Equation~\ref{eq:iterate} represents the proximal gradient method for direct MC minimization.
\subsection{Low Dimensional Example}
\begin{figure}[!h]
\centering
\includegraphics[width=2.4in]{lowdimhighmc.eps}
\caption{A linearly dependant spanning set for $\mathbb{R}^2$.}
\label{Fig:lowdimhigh}
\end{figure}
\noindent The following example will help illustrate the application of mutual coherence minimization
to an overcomplete dictionary. The dictionary is chosen to be low dimensional so the concept
of mutual coherence minimization can be illustrated clearly.
Consider $D \in \mathbb{R}^{m\times n}$:
\begin{align}
    D = \begin{bmatrix} 0 &1 &\>\>\>\sqrt{2}/2 \\
                        1 &0 &-\sqrt{2}/2 \end{bmatrix}.
\end{align}
Shown in Fig. \ref{Fig:lowdimhigh}, $D$ will have its mutual coherence minimized through proximal
gradient descent.
The chosen dictionary has initial MC and Welch bound:
\begin{align}
\mu (D) &= \sqrt{2}/2 \\
\mu_{WB} (D) &= 0.5
\end{align}
After application of the algorithm of Lin et al. to the matrix, a new matrix is returned:
\begin{align}
    D = \left[\begin{array}{ccc} 0.966 & -0.258  &0.707\\ -0.259 & 0.966 & 0.707 \end{array}\right],
\end{align}
and the new MC:
\begin{align}
\mu (M) &= 0.500. 
\end{align}

This new dictionary is shown in Fig.~\ref{Fig:lowdimlow}
Note that any unitary transformation (rotation) of this matrix will also have the minimum possible MC for
matrices in $\mathbb{R}^{2 \times 3}$.
\begin{figure}[!t]
\centering
\includegraphics[width=2.4in]{lowdimlowmc.eps}
\caption{A less linearly dependant spanning set for $\mathbb{R}^2$. The absolute value
of the inner product has been minimized for this number of vectors in $\mathbb{R}^2$.}
\label{Fig:lowdimlow}
\end{figure}
It should be seen that in this case in $\mathbb{R}^{2\times3}$ the Welch bound for MC of 0.5 is 
actually achieved. This is not always the case however as will be shown in the next example.
%\pagebreak \\
The algorithm of Lin et al. quickly reduces the mutual coherence of the dictionary in the first iteration while then taking some time to settle to a final solution (Fig.~\ref{Fig:mcvk}).
\begin{figure}[!t]
\centering
\includegraphics[width=2.4in]{MCvk.eps}
\caption{The MC falls quickly in the first iteration before oscillating wildly on its way towards a
steady solution. These oscillations are more prevalent in lower dimensional problems.}
\label{Fig:mcvk}
\end{figure}

\subsection{Minimizing Mutual Coherence with Projection}
In this section the minimization of mutual coherence of matrix $M$, which is product of dictionary $D$ and projection operator $P$, is discussed. Finding of an optional projection matrix is important for CS application. In this case we consider the following problem
\begin{align}
\min_{P\in\mathbb{R}^{m\times n}}f(PD)=\|(PD)^T(PD)-I\|_{\infty}\\
\textrm{s.t.}\quad\|(PD)_i\|_2=1.
\end{align}
The way of solving this problem is similar to the MC minimization of the dictionary. 
 The following pseudo-code summarizes the algorithm workflow used in detail.

\textit{The algorithm of Lin et al.}\\
\textbf{Initialize:} $k=0$, $K>0$, $t=0$, $T>0$, $\rho>0$,  $\alpha=0.9\rho$, $\eta>1$, $\beta>0$, $D\in\mathbb{R}^{d\times n}$, $M_k\in\mathbb{R}^{m\times n}$, $P_k\in\mathbb{R}^{m\times d}$.

\textbf{Output:} $P_{\textrm{small}\;\mu}$\\
\textbf{while} $t>T$ \\
\textbf{while} $k>K$ 
\begin{enumerate}
     \item Compute $P_k$ by $P_k=M_kD^{\dag}$
    \item Compute $V_k$ by solving 
\begin{align}
V_k=\arg\min_{V}\frac{1}{2}\| V-(M_k^TM_k-I)/\rho  \|_F
\end{align}
\begin{align}
\textrm{subject}\;\textrm{to} \;\|(V)\|_1\leq1
\end{align}
using the algorithm from \cite{l1_ball}.
    \item Compute $\nabla f_{\rho}(M_k)=M_k(V_k+V^T_k)$
    \item Compute $P_kD$
    \item Compute $M_{k+1}$ by using the closed form procedure for the proximal gradient method 
\begin{align}
(M_{k+1})_i=\frac{z}{\|z\|_2},
\end{align}
where $a$ is
\begin{align}
    z = \frac{\frac{1}{\alpha}(M_k)_i + \frac{1}{\beta}(P_k D)_i - (\nabla f_{\rho}( M_k)_i)}{\frac{1}{\alpha}+\frac{1}{\beta}}
\end{align}
    \item $k=k+1$
\end{enumerate}
\textbf{end while}
\begin{enumerate}
    \item Update the smoothing parameters: $\rho=\rho/\eta$, $\alpha=0.9\rho$, $\beta=\beta/\eta$,
    \item $t=t+1$
\end{enumerate}
\textbf{end while}


It is worth to mention that the dictionary $D$ is kept fixed, while the projection operator $P$ is updated by means of iterating $M$. First, the projection operator is chosen as a random matrix, which is built using the uniform distribution (Fig.~\ref{Fig:initial_rand}). As for the dictionary, the Fourier basis functions are chosen. Consequently, $D\in\mathbb{C}^{n\times n}$ and $D^{\dag}=D^{H}$ . It turns out that all other matrices will be complex as well, only one change is needed in the algorithm above: all transpose operations must be replaced with conjugate transpose. The Fig.~\ref{Fig:opt_rand} shows the result of the minimization algorithm and Fig.~\ref{Fig:mc_rand} presents the MC versus iteration. After the 90th iteration MC value stays constant. The Welch bound is not achieved. It is assumed that such behavior of MC is not due to an artifact of limitations of the algorithm. 

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{initial_fp.eps}
\caption{The initial projection matrix.}
\label{Fig:initial_rand}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{opt_fp.eps}
\caption{The optimized projection matrix}
\label{Fig:opt_rand}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{diff_fp.eps}
\caption{The difference between the initial and optimized projection matrix.}
\label{Fig:diff_rand}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=3.0in]{mc_fp1.eps}
\caption{The mutual coherence updating graph. With increasing number of iterations the mutual coherence starts decay very slowly.}
\label{Fig:mc_rand}
\end{figure}

Another example of projection operator and its minimization are presented in Fig.~\ref{Fig:initial},~\ref{Fig:opt},~\ref{Fig:diff}, and ~\ref{Fig:mc}. The idea of this projection matrix is to represent the positions of sensors. The entry equal to 1 represent the presents of sensor. The entry equal 0 reports that there is no sensor at this location. The result of optimization (Fig.~\ref{Fig:opt}) shows the values changes of non-zero entries, but it does not change the location of the non-zero entry within the matrix. Therefore, for those who are interested in optimizing the position of sensors, it is recommended to address another paper \cite{Obermeier}.


\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{initial_fp_n1.eps}
\caption{The initial projection matrix.}
\label{Fig:initial}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{opt_fp_n1.eps}
\caption{The optimized projection matrix.}
\label{Fig:diff}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{diff_fp_n1.eps}
\caption{The difference between the initial and optimized projection matrix.}
\label{Fig:opt}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{mc_fp_n1.eps}
\caption{The mutual coherence updating graph. With increasing number of iterations the mutual coherence starts decay very slowly. The Welch bound is the same as for Fig.~\ref{Fig:mc_rand} because the matrices have the same size.}
\label{Fig:mc}
\end{figure}


\section{Results}
The algorithm of Lin et al. is an efficient way to minimize the MC of dictionaries and projection operators used
in Compressive Sensing applications. When compared to other current algorithms, it currently
outperforms the others in terms of the convergence rate and the minimization value achieved.
\subsection{Problems with stability}
The only time the algorithm could not perform well was
when it was given poorly conditioned input. This includes cases where where the initial MC of the
dictionary is too close to 1 either because two or more vectors are nearly parallel or opposite. Under these circumstances the algorithm is unable to compute the gradient of the dictionary in a stable manner.

\subsection{Improving stability}
Restricting the dictionaries which are attempted to be optimized to those which do not 
include redundant or exactly opposite entries is an effective technique for maintaining stability 
when using the algorithm of Lin et al.
\section{Conclusion}
The field of CS is rapidly evolving at this time, and this is evidenced by the progress shown
in algorithms such as those developed by Lin et al. and others. The technique discussed in this paper 
provides a way to enhance the dictionary used in a CS application in a flexible manner. First, the 
underlying dictionary can be modified for better mutual coherence. Secondly, if the underlying dictionary is fixed, the 
mutual coherence of the overall dictionary may be reduced through the application of a projection
matrix.

These bold new techniques afford more opportunity to researchers as they pursue sparser and sparser
representations for signals.

\begin{thebibliography}{9}
\bibitem{Lin} Z. Lin, C. Lu, and H. Li,  ``Optimized projections for compressed sensing via direct mutual coherence minimization’’, Signal Processing, vol.~151, pp.~45-55, 2018

\bibitem{Elard} 
M. Elad, “Optimized projections for compressed sensing,” IEEE Transactions on Signal Processing, vol.~55, no.~12, pp.~5695–5702, 2007.

\bibitem{Sapiro}  J. M. Duarte-Carvajalino and G. Sapiro, “Learning to sense sparse signals: Simultaneous sensing matrix and sparsifying dictionary optimization,” IEEE Transactions on Image Processing, vol.~18, no.~7, pp.~1395–1408, 2009.

\bibitem{Xu} 
J. Xu, Y. Pi, and Z. Cao, “Optimized projection matrix for compressive sensing,” EURASIP Journal on Advances in Signal Processing, vol.~2010, pp.~43, 2010.
 
\bibitem{Obermeier}  R. Obermeier and J. A. Martinez-Lorenzo, ``Sensing Matrix Design via Mutual Coherence Minimization for Electromagnetic Compressive Imaging Applications,” IEEE  Transactions on Computational Imaging, vol.~3, no.~2, pp.~2333-9403, 2017

\bibitem{l1_ball} 
Y. Singer, J. Duchi, S. Shalev-Shwartz, and T. Chandra, ``Efficient projections onto the l1-ball for learning in high dimensions,” in International Conference of Machine Learning, 2008.





\end{thebibliography}

\end{document}
